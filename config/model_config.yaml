# Model Configuration

# XGBoost Model Parameters
xgboost:
  objective: "reg:squarederror"
  n_estimators: 100
  max_depth: 6
  learning_rate: 0.1
  subsample: 0.8
  colsample_bytree: 0.8
  gamma: 0
  min_child_weight: 1
  reg_alpha: 0
  reg_lambda: 1
  random_state: 42
  n_jobs: -1
  tree_method: "hist"  # "gpu_hist" for GPU acceleration

# LightGBM Model Parameters
lightgbm:
  objective: "regression"
  n_estimators: 100
  max_depth: 6
  learning_rate: 0.1
  num_leaves: 31
  subsample: 0.8
  colsample_bytree: 0.8
  min_child_samples: 20
  reg_alpha: 0
  reg_lambda: 1
  random_state: 42
  n_jobs: -1
  device: "cpu"  # "gpu" for GPU acceleration

# Random Forest Model Parameters
random_forest:
  n_estimators: 100
  max_depth: 10
  min_samples_split: 2
  min_samples_leaf: 1
  max_features: "sqrt"
  bootstrap: true
  random_state: 42
  n_jobs: -1

# LSTM Model Parameters (Phase 2)
lstm:
  sequence_length: 60
  lstm_units: [128, 64]
  dropout: 0.2
  dense_units: [32, 16]
  optimizer: "adam"
  learning_rate: 0.001
  batch_size: 32
  epochs: 100
  validation_split: 0.2
  early_stopping_patience: 10

# GRU Model Parameters (Phase 2)
gru:
  sequence_length: 60
  gru_units: [128, 64]
  dropout: 0.2
  dense_units: [32, 16]
  optimizer: "adam"
  learning_rate: 0.001
  batch_size: 32
  epochs: 100
  validation_split: 0.2
  early_stopping_patience: 10

# Hyperparameter Optimization Settings
optuna:
  n_trials: 50
  timeout: 3600  # 1 hour
  n_jobs: -1

  # Parameter search space
  xgboost_search_space:
    n_estimators: [50, 100, 200, 300]
    max_depth: [3, 5, 7, 9]
    learning_rate: [0.01, 0.05, 0.1, 0.2]
    subsample: [0.6, 0.7, 0.8, 0.9, 1.0]
    colsample_bytree: [0.6, 0.7, 0.8, 0.9, 1.0]
    gamma: [0, 0.1, 0.2, 0.3]
    min_child_weight: [1, 3, 5, 7]

# Ensemble Model Settings
ensemble:
  method: "voting"  # "voting" or "stacking"
  voting_type: "soft"  # "soft" or "hard"

  # Models to include in ensemble
  models:
    - "xgboost"
    - "lightgbm"
    - "random_forest"

  # Weights for voting (if None, equal weights)
  weights: null

  # Stacking meta-learner
  meta_learner: "xgboost"
